{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"强化学习专题 - 价值迭代算法实践\"\n",
    "date: 2025-06-04T03:58:00+08:00\n",
    "author: \"Liu Zheng\"\n",
    "tags: [\"笔记\", \"实验\", \"AI\", \"强化学习\"]\n",
    "categories: \"实验笔记\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值迭代算法实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基本概念\n",
    "\n",
    "价值迭代（Value Iteration）是一种用于求解马尔可夫决策过程（MDP）最优策略的动态规划算法。其核心思想是通过反复更新每个状态的价值估计，逐步逼近最优状态值函数 $V^*(s)$，进而导出最优策略 $\\pi^*(s)$。\n",
    "\n",
    "价值迭代算法能够高效地找到最优价值函数和最优策略，适用于状态空间较小且模型已知的情形。\n",
    "\n",
    "> **算法流程**：\n",
    "> \n",
    "> $\\begin{array}{l}\n",
    ">   input:\\ \\text{MDP}\\ = \\{ S, A, s_0, P(s' \\mid s, a), R(s,a,s') \\}\\\\\n",
    ">   output:\\ \\text{Value Function}\\ V\\\\[4mm]\n",
    ">   \\text{Set}\\ V\\ \\text{To Arbitrary Value Function; }\\ V(s) = 0\\ \\text{for all}\\ s\\\\[4mm]\n",
    ">   repeat\\ \\\\\n",
    ">   \\quad\\quad \\Delta \\leftarrow 0 \\\\\n",
    ">   \\quad\\quad foreach\\ s \\in S \\\\\n",
    ">   \\quad\\quad\\quad\\quad V'(s) \\leftarrow \\max_{a \\in A(s)} \\sum_{s' \\in S}  P(s' \\mid s, a)\\ [R(s,a,s') + \\gamma\\ V(s') ] \\\\\n",
    ">   \\quad\\quad\\quad\\quad \\Delta \\leftarrow \\max(\\Delta, |V'(s) - V(s)|) \\\\\n",
    ">   \\quad\\quad V \\leftarrow V' \\\\\n",
    ">   until\\ \\Delta \\leq \\theta \n",
    "> \\end{array}\n",
    "> $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例场景\n",
    "\n",
    "![Value-based](https://gibberblot.github.io/rl-notes/_images/9322df0c1c9c3378ee27b457ed43bad878f17fd29ae92fd027efe05f7c835593.png)\n",
    "\n",
    "**1. 网格（Grid）：**\n",
    "- 这是一个3x3的方格，总共有9个单元格。\n",
    "- 蓝色圆圈：表示智能体（Agent）的起始位置，在左下角。\n",
    "- 灰色方块：表示墙壁。智能体不能进入这个单元格。\n",
    "- 绿色方块：表示目标单元格，智能体到达这里会获得 +1.00 的奖励（Reward）。\n",
    "- 红色方块：表示一个危险单元格，智能体到达这里会获得 -1.00 的奖励（惩罚）。\n",
    "- 白色方块：表示普通单元格，到达这些单元格没有即时奖励。\n",
    "**2. 行动（Actions）和不确定性（Uncertainty）：**\n",
    "- 智能体可以尝试向北、南、东、西四个方向移动。\n",
    "- 但关键在于，智能体的行动不总是成功的。这是这个问题的核心难点。\n",
    "- 以“尝试向北移动”为例：\n",
    "    - 80% 的时间：智能体成功向北移动（如果北边没有墙）。\n",
    "    - 10% 的时间：智能体意外地向西移动（如果西边没有墙）。\n",
    "    - 10% 的时间：智能体意外地向东移动（如果东边没有墙）。\n",
    "- 墙壁规则： 如果智能体尝试移动的方向有墙壁阻挡，它会停留在当前单元格。这个规则适用于所有方向，不仅仅是向北。\n",
    "**3. 目标（Goal）：**\n",
    "- 任务是从左下角的起始单元格出发，找到一个“最佳行动序列”（或者更准确地说，是一个策略），使得智能体能够最大化它所能获得的“预期奖励”（Expected Reward）。\n",
    "- “预期奖励”意味着我们需要考虑行动的不确定性，计算出在各种可能结果下的平均奖励。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
